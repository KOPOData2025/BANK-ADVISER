#!/usr/bin/env python3
"""
Voice Profile ê¸°ë°˜ í™”ì ë¶„ë¦¬ ì„œë²„ (5005 í¬íŠ¸)
- voice_profiles í…Œì´ë¸”ì˜ í–‰ì› ëª©ì†Œë¦¬ì™€ ë¹„êµí•˜ì—¬ ì •í™•ë„ í–¥ìƒ
- í–‰ì› ëª©ì†Œë¦¬ì™€ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ë§Œ 'employee'ë¡œ ë¶„ë¥˜
- ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ 'customer'ë¡œ ë¶„ë¥˜
"""

import os
import sys
import json
import logging
import requests
import librosa
from scipy.io import wavfile as scipy_wavfile
import numpy as np
from flask import Flask, request, jsonify
from flask_cors import CORS
from datetime import datetime
import joblib
import subprocess
import shutil
from sklearn.metrics.pairwise import cosine_similarity
from typing import Optional
from sklearn.feature_extraction.text import TfidfVectorizer
import re
import base64

# Optional: Whisper transcription for overlap
whisper_model = None
try:
    from faster_whisper import WhisperModel  # type: ignore
except Exception:
    WhisperModel = None  # Lazy import fallback

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

# Supabase ì„¤ì •
SUPABASE_URL = "https://jhfjigeuxrxxbbsoflcd.supabase.co"
SUPABASE_ANON_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImpoZmppZ2V1eHJ4eGJic29mbGNkIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTYxMDA1OTksImV4cCI6MjA3MTY3NjU5OX0.aCXzQYf1P1B2lVHUfDlLdjB-iq-ItlPRh6oWRTElRUQ"

# ì „ì—­ ë³€ìˆ˜
voice_profiles = {}
custom_classifier = None
label_mapping = {}
pyannote_model = None

def load_voice_profiles():
    """Supabaseì—ì„œ voice_profiles ë°ì´í„° ë¡œë“œ"""
    global voice_profiles
    
    try:
        headers = {
            "apikey": SUPABASE_ANON_KEY,
            "Authorization": f"Bearer {SUPABASE_ANON_KEY}",
            "Content-Type": "application/json"
        }
        
        logger.info(f"ğŸ” Supabase ìš”ì²­: {SUPABASE_URL}/rest/v1/voice_profiles")
        response = requests.get(
            f"{SUPABASE_URL}/rest/v1/voice_profiles",
            headers=headers
        )
        
        logger.info(f"ğŸ“¡ ì‘ë‹µ ìƒíƒœ: {response.status_code}")
        logger.info(f"ğŸ“„ ì‘ë‹µ ë‚´ìš©: {response.text[:500]}...")
        
        if response.status_code == 200:
            profiles = response.json()
            voice_profiles = {}
            
            for profile in profiles:
                employee_id = profile.get('employee_id')
                audio_url = profile.get('audio_file_url')
                confidence = float(profile.get('confidence_score', 0.9))
                
                if employee_id and audio_url:
                    voice_profiles[employee_id] = {
                        'audio_url': audio_url,
                        'confidence': confidence,
                        'employee_name': profile.get('employee_name', 'Unknown')
                    }
            
            logger.info(f"âœ… Voice profiles ë¡œë“œ ì™„ë£Œ: {len(voice_profiles)}ê°œ")
            for emp_id, profile in voice_profiles.items():
                logger.info(f"  - {emp_id}: {profile['employee_name']} (ì‹ ë¢°ë„: {profile['confidence']})")
            
            return True
        else:
            logger.error(f"âŒ Voice profiles ë¡œë“œ ì‹¤íŒ¨: {response.status_code}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ Voice profiles ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False

def initialize_custom_model():
    """ì»¤ìŠ¤í…€ ëª¨ë¸ ì´ˆê¸°í™”"""
    global custom_classifier, label_mapping, pyannote_model
    
    try:
        # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì„¤ì • (EC2 ì¹œí™”ì  ê²½ë¡œ + í™˜ê²½ë³€ìˆ˜ ì˜¤ë²„ë¼ì´ë“œ ì§€ì›)
        base_dir = os.getenv(
            "CUSTOM_MODEL_DIR",
            os.path.join(os.path.dirname(__file__), "second_train")
        )
        model_path = os.path.join(base_dir, "speaker_role_classifier.joblib")
        label_path = os.path.join(base_dir, "label_mapping.json")
        
        if os.path.exists(model_path) and os.path.exists(label_path):
            custom_classifier = joblib.load(model_path)
            
            with open(label_path, 'r', encoding='utf-8') as f:
                label_mapping = json.load(f)
            
            # pyannote.audio ëª¨ë¸ ì´ˆê¸°í™” (í•™ìŠµëœ ë¶„ë¥˜ê¸° ì‚¬ìš©ì„ ìœ„í•´)
            try:
                from pyannote.audio import Model
                pyannote_model = Model.from_pretrained("pyannote/embedding", use_auth_token=os.getenv("HUGGINGFACE_TOKEN"))
                logger.info("âœ… pyannote.audio ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ pyannote.audio ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                pyannote_model = None
            
            logger.info("âœ… ì»¤ìŠ¤í…€ ë¶„ë¥˜ê¸° ë¡œë“œ ì™„ë£Œ")
            logger.info(f"âœ… ë¼ë²¨ ë§¤í•‘ ë¡œë“œ ì™„ë£Œ: {label_mapping}")
            return True
        else:
            logger.info("â„¹ï¸ ì»¤ìŠ¤í…€ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Voice Profile ê¸°ë°˜ ë¶„ë¥˜ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            custom_classifier = None
            label_mapping = {}
            pyannote_model = None
            return True
            
    except Exception as e:
        logger.error(f"âŒ ì»¤ìŠ¤í…€ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        return False

def extract_audio_features(audio_url):
    """ì˜¤ë””ì˜¤ URLì—ì„œ íŠ¹ì§• ì¶”ì¶œ (scipyë¡œ ë¡œë“œ â†’ librosaë¡œ MFCC)"""
    temp_file = None
    temp_wav = None
    try:
        logger.info(f"ğŸµ ì˜¤ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ ì‹œì‘: {audio_url}")

        # ì˜¤ë””ì˜¤ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        response = requests.get(audio_url, timeout=15)
        if response.status_code != 200:
            logger.error(f"âŒ ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {response.status_code}")
            return None

        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        temp_file = "/tmp/temp_audio.wav"
        with open(temp_file, 'wb') as f:
            f.write(response.content)

        logger.info(f"ğŸ“ ì„ì‹œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {temp_file}")

        # ë¨¼ì € WAVë¡œ ê°€ì •í•˜ê³  scipyë¡œ ë¡œë“œ ì‹œë„
        try:
            sr_native, y = scipy_wavfile.read(temp_file)
            logger.info(f"ğŸµ scipy ë¡œë“œ ì™„ë£Œ: dtype={getattr(y, 'dtype', None)}, shape={getattr(y, 'shape', None)}, sr={sr_native}")
        except Exception as e:
            logger.warning(f"âš ï¸ ì§ì ‘ WAV ë¡œë“œ ì‹¤íŒ¨, ffmpeg ë³€í™˜ ì‹œë„: {str(e)}")
            # íŒŒì¼ì´ WebM/Opus ë“±ì¸ ê²½ìš° ffmpegë¡œ PCM WAV ë³€í™˜
            if shutil.which('ffmpeg') is None:
                logger.error("âŒ ffmpeg ë¯¸ì„¤ì¹˜ - ë¹„WAV íŒŒì¼ ë³€í™˜ ë¶ˆê°€")
                return None
            temp_wav = "/tmp/temp_converted.wav"
            try:
                cmd = [
                    'ffmpeg', '-y', '-hide_banner', '-loglevel', 'error',
                    '-i', temp_file,
                    '-ar', '16000', '-ac', '1', '-f', 'wav', temp_wav
                ]
                subprocess.run(cmd, check=True)
                sr_native, y = scipy_wavfile.read(temp_wav)
                logger.info(f"ğŸµ ffmpeg ë³€í™˜ í›„ ë¡œë“œ ì™„ë£Œ: dtype={getattr(y, 'dtype', None)}, shape={getattr(y, 'shape', None)}, sr={sr_native}")
            except subprocess.CalledProcessError as ce:
                logger.error(f"âŒ ffmpeg ë³€í™˜ ì‹¤íŒ¨: {str(ce)}")
                return None

        # ìœ íš¨ì„± ê²€ì‚¬
        if y is None:
            logger.error("âŒ ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤")
            return None

        # ìŠ¤í…Œë ˆì˜¤ â†’ ëª¨ë…¸
        if hasattr(y, 'ndim') and y.ndim == 2:
            y = y.mean(axis=1)

        # ì •ìˆ˜í˜•ì„ float32 [-1, 1]ë¡œ ì •ê·œí™”
        if np.issubdtype(y.dtype, np.integer):
            max_val = np.iinfo(y.dtype).max
            if max_val == 0:
                logger.error("âŒ ì˜¤ë””ì˜¤ ì •ê·œí™” ì‹¤íŒ¨: max_val=0")
                return None
            y = y.astype(np.float32) / max_val
        else:
            y = y.astype(np.float32)

        # ë¦¬ìƒ˜í”Œë§ (í•„ìš” ì‹œ)
        target_sr = 16000
        if sr_native != target_sr:
            y = librosa.resample(y, orig_sr=sr_native, target_sr=target_sr)
            sr = target_sr
        else:
            sr = sr_native

        logger.info(f"ğŸ¼ ì‹ í˜¸ ì¤€ë¹„ ì™„ë£Œ: ê¸¸ì´={len(y)}, sr={sr}")

        # ìµœì†Œ ê¸¸ì´ ì²´í¬ (ì§§ì€ ì‹ í˜¸ëŠ” MFCC ë¶ˆê°€)
        if len(y) < sr * 0.2:  # 200ms ë¯¸ë§Œì´ë©´ ìŠ¤í‚µ
            logger.warning("âš ï¸ ì˜¤ë””ì˜¤ê°€ ë„ˆë¬´ ì§§ì•„ MFCC ì¶”ì¶œ ë¶ˆê°€")
            return None

        # MFCC ì¶”ì¶œ
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        mfccs_mean = np.mean(mfccs, axis=1)

        logger.info(f"ğŸ¯ MFCC íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ: {len(mfccs_mean)} ì°¨ì›")
        return mfccs_mean

    except Exception as e:
        logger.error(f"âŒ ì˜¤ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        return None
    finally:
        try:
            if temp_file and os.path.exists(temp_file):
                os.remove(temp_file)
            if temp_wav and os.path.exists(temp_wav):
                os.remove(temp_wav)
        except Exception:
            pass

def extract_pyannote_embedding(audio_file_path: str) -> Optional[np.ndarray]:
    """WAV íŒŒì¼ì—ì„œ pyannote.audio ì„ë² ë”© ì¶”ì¶œ (í•™ìŠµëœ ë¶„ë¥˜ê¸°ìš©)"""
    global pyannote_model
    
    if pyannote_model is None:
        logger.warning("âš ï¸ pyannote.audio ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
        return None
    
    try:
        import torch
        import soundfile as sf
        
        # ì˜¤ë””ì˜¤ ë¡œë“œ
        audio, sr = sf.read(audio_file_path)
        
        # 1ì´ˆ ë¯¸ë§Œì˜ ì§§ì€ ì˜¤ë””ì˜¤ëŠ” ê±´ë„ˆë›°ê¸°
        if len(audio) < sr:
            logger.warning("âš ï¸ ì˜¤ë””ì˜¤ê°€ ë„ˆë¬´ ì§§ìŒ (1ì´ˆ ë¯¸ë§Œ)")
            return None
        
        # ìŠ¤í…Œë ˆì˜¤ â†’ ëª¨ë…¸
        if audio.ndim > 1:
            audio = np.mean(audio, axis=1)
        
        # torch í…ì„œë¡œ ë³€í™˜
        waveform = torch.tensor(audio, dtype=torch.float32).unsqueeze(0)
        
        # pyannote.audio ì„ë² ë”© ì¶”ì¶œ
        with torch.no_grad():
            embedding = pyannote_model(waveform).squeeze(0).cpu().numpy()
        
        logger.info(f"ğŸ¯ pyannote.audio ì„ë² ë”© ì¶”ì¶œ ì™„ë£Œ: {len(embedding)} ì°¨ì›")
        return embedding
        
    except Exception as e:
        logger.error(f"âŒ pyannote.audio ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        return None

def extract_audio_features_from_base64(audio_base64: str, mime: Optional[str] = None):
    """í´ë¼ì´ì–¸íŠ¸ì—ì„œ ë³´ë‚´ì˜¨ base64 ì˜¤ë””ì˜¤(webm ë“±)ë¥¼ WAVë¡œ ë³€í™˜ í›„ MFCC ì¶”ì¶œ"""
    temp_src = None
    temp_wav = None
    try:
        if not audio_base64:
            return None
        if shutil.which('ffmpeg') is None:
            logger.error("âŒ ffmpeg ë¯¸ì„¤ì¹˜ - base64 ì˜¤ë””ì˜¤ ë³€í™˜ ë¶ˆê°€")
            return None

        # ë””ì½”ë“œí•˜ì—¬ ì„ì‹œ ì†ŒìŠ¤ íŒŒì¼ë¡œ ì €ì¥ (webm/ogg/opus ë“±)
        raw = base64.b64decode(audio_base64)
        ext = 'webm'
        if mime and 'ogg' in mime:
            ext = 'ogg'
        temp_src = f"/tmp/snippet_src.{ext}"
        with open(temp_src, 'wb') as f:
            f.write(raw)

        # ffmpegë¡œ 16kHz mono WAV ë³€í™˜
        temp_wav = "/tmp/snippet_conv.wav"
        cmd = [
            'ffmpeg', '-y', '-hide_banner', '-loglevel', 'error',
            '-i', temp_src,
            '-ar', '16000', '-ac', '1', '-f', 'wav', temp_wav
        ]
        subprocess.run(cmd, check=True)

        # WAV ë¡œë“œ
        sr_native, y = scipy_wavfile.read(temp_wav)
        if hasattr(y, 'ndim') and y.ndim == 2:
            y = y.mean(axis=1)
        if np.issubdtype(y.dtype, np.integer):
            max_val = np.iinfo(y.dtype).max
            if max_val == 0:
                return None
            y = y.astype(np.float32) / max_val
        else:
            y = y.astype(np.float32)

        sr = sr_native
        if sr != 16000:
            y = librosa.resample(y, orig_sr=sr, target_sr=16000)
            sr = 16000

        if len(y) < sr * 0.2:
            logger.warning("âš ï¸ base64 ì˜¤ë””ì˜¤ê°€ ë„ˆë¬´ ì§§ì•„ MFCC ì¶”ì¶œ ë¶ˆê°€")
            return None

        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        mfccs_mean = np.mean(mfccs, axis=1)
        return mfccs_mean
    except Exception as e:
        logger.error(f"âŒ base64 ì˜¤ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        return None
    finally:
        try:
            if temp_src and os.path.exists(temp_src):
                os.remove(temp_src)
            if temp_wav and os.path.exists(temp_wav):
                os.remove(temp_wav)
        except Exception:
            pass

def calculate_voice_similarity(audio_features1, audio_features2):
    """ë‘ ì˜¤ë””ì˜¤ íŠ¹ì§• ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
    try:
        if audio_features1 is None or audio_features2 is None:
            return 0.0
            
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = cosine_similarity([audio_features1], [audio_features2])[0][0]
        return float(similarity)
        
    except Exception as e:
        logger.error(f"âŒ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
        return 0.0

def classify_speaker_by_voice(transcript, audio_features=None, employee_voice_profile=None):
    """ìŒì„± íŠ¹ì§• ê¸°ë°˜ í™”ì ë¶„ë¥˜"""
    global voice_profiles
    
    logger.info(f"ğŸ” ìŒì„± ê¸°ë°˜ ë¶„ë¥˜ ì‹œì‘ - audio_features: {audio_features is not None}, employee_voice_profile: {employee_voice_profile is not None}")
    
    if not voice_profiles:
        logger.info("âš ï¸ Voice profiles ì—†ìŒ - í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ë¥˜ë¡œ í´ë°±")
        text_result = classify_speaker_by_text(transcript)
        text_result.update({
            'similarity': 0.0,
            'decision_reason': 'text_no_profiles',
            'overlap': False
        })
        return text_result
    
    # ì‹¤ì œ ìŒì„± ë°ì´í„°ê°€ ìˆìœ¼ë©´ MFCC íŠ¹ì§• ì¶”ì¶œ ì‹œë„
    if audio_features is None and employee_voice_profile:
        try:
            logger.info("ğŸµ ìŒì„± íŠ¹ì§• ì¶”ì¶œ ì‹œë„...")
            audio_features = extract_audio_features(employee_voice_profile.get('audio_file_url'))
            if audio_features is not None:
                logger.info(f"âœ… ìŒì„± íŠ¹ì§• ì¶”ì¶œ ì„±ê³µ: {len(audio_features)} ì°¨ì›")
            else:
                logger.warning("âŒ ìŒì„± íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨")
        except Exception as e:
            logger.error(f"âŒ ìŒì„± íŠ¹ì§• ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    if audio_features is None:
        logger.info("âš ï¸ ìŒì„± íŠ¹ì§• ì—†ìŒ - í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ë¥˜ë¡œ í´ë°±")
        text_result = classify_speaker_by_text(transcript)
        text_result.update({
            'similarity': 0.0,
            'decision_reason': 'text_no_features',
            'overlap': False
        })
        return text_result
    
    best_match = None
    best_similarity = 0.0
    similarity_threshold = 0.7  # ìœ ì‚¬ë„ ì„ê³„ê°’
    secondary = None
    secondary_similarity = 0.0
    
    # ê° í–‰ì›ì˜ ìŒì„± í”„ë¡œí•„ê³¼ ë¹„êµ
    for employee_id, profile in voice_profiles.items():
        try:
            # í–‰ì› ìŒì„± íŠ¹ì§• ì¶”ì¶œ
            employee_features = extract_audio_features(profile['audio_url'])
            
            if employee_features is not None:
                # ìœ ì‚¬ë„ ê³„ì‚°
                similarity = calculate_voice_similarity(audio_features, employee_features)
                
                logger.info(f"ğŸ” {employee_id} ({profile['employee_name']}) ìœ ì‚¬ë„: {similarity:.3f}")
                
                # best, secondary ì¶”ì 
                if similarity > best_similarity:
                    secondary, secondary_similarity = best_match, best_similarity
                    best_similarity = similarity
                    best_match = {
                        'employee_id': employee_id,
                        'employee_name': profile['employee_name'],
                        'similarity': similarity
                    }
                elif similarity > secondary_similarity:
                    secondary = {
                        'employee_id': employee_id,
                        'employee_name': profile['employee_name'],
                        'similarity': similarity
                    }
                    secondary_similarity = similarity
        
        except Exception as e:
            logger.error(f"âŒ {employee_id} ìŒì„± ë¹„êµ ì‹¤íŒ¨: {str(e)}")
            continue
    
    # ê¸°ë³¸ overlap ê°ì§€: ê²½ê³„ êµ¬ê°„(0.65~0.80)ì—ì„œ ê²¹ì¹¨ í”Œë˜ê·¸ë§Œ í‘œì‹œ
    overlap = False
    decision_reason = 'voice'
    if 0.65 <= best_similarity < 0.80:
        overlap = True
        decision_reason = 'voice_overlap_threshold'
    
    if best_match and best_similarity > similarity_threshold:
        logger.info(f"âœ… ìŒì„± ë§¤ì¹­ ì„±ê³µ: {best_match['employee_name']} (ìœ ì‚¬ë„: {best_match['similarity']:.3f})")
        return {
            'speaker_id': f"speaker_{best_match['employee_id']}",
            'speaker_name': 'employee',
            'confidence': best_similarity,
            'employee_id': best_match['employee_id'],
            'employee_name': best_match['employee_name'],
            'similarity': best_similarity,
            'decision_reason': decision_reason,
            'overlap': overlap,
            'secondary_speaker': None if not overlap else 'customer'
        }
    else:
        logger.info("ğŸ‘¤ ìŒì„± ë§¤ì¹­ ì‹¤íŒ¨ - ê³ ê°ìœ¼ë¡œ ë¶„ë¥˜")
        return {
            'speaker_id': 'speaker_customer',
            'speaker_name': 'customer',
            'confidence': max(0.7, 1.0 - best_similarity),
            'employee_id': None,
            'employee_name': None,
            'similarity': best_similarity,
            'decision_reason': 'voice_no_match',
            'overlap': overlap,
            'secondary_speaker': None
        }

def classify_speaker_by_pyannote_embedding(audio_file_path: str) -> dict:
    """pyannote.audio ì„ë² ë”©ìœ¼ë¡œ í•™ìŠµëœ ë¶„ë¥˜ê¸° ì‚¬ìš©"""
    global custom_classifier, label_mapping
    
    if not custom_classifier or not label_mapping:
        logger.warning("âš ï¸ í•™ìŠµëœ ë¶„ë¥˜ê¸°ê°€ ì—†ìŒ")
        return None
    
    try:
        # pyannote.audio ì„ë² ë”© ì¶”ì¶œ
        embedding = extract_pyannote_embedding(audio_file_path)
        if embedding is None:
            logger.warning("âš ï¸ pyannote.audio ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨")
            return None
        
        # í•™ìŠµëœ ë¶„ë¥˜ê¸°ë¡œ ì˜ˆì¸¡
        prediction = custom_classifier.predict([embedding])[0]
        
        # ë¼ë²¨ ë§¤í•‘ì—ì„œ í™”ì ì´ë¦„ ì°¾ê¸°
        speaker_name = None
        for name, label in label_mapping.items():
            if label == prediction:
                speaker_name = name
                break
        
        if speaker_name:
            # ì˜ˆì¸¡ í™•ë¥ ë„ í•¨ê»˜ ê³„ì‚°
            prediction_proba = custom_classifier.predict_proba([embedding])[0]
            max_confidence = float(np.max(prediction_proba))
            
            logger.info(f"ğŸ¯ í•™ìŠµëœ ë¶„ë¥˜ê¸° ì˜ˆì¸¡ ê²°ê³¼: {speaker_name} (ì‹ ë¢°ë„: {max_confidence:.3f})")
            
            # ì‹ ë¢°ë„ê°€ 0.8 ì´ìƒì¼ ë•Œë§Œ ì‚¬ìš©, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ None ë°˜í™˜í•˜ì—¬ í´ë°±
            if max_confidence >= 0.8:
                return {
                    'speaker_id': f"speaker_{speaker_name}",
                    'speaker_name': speaker_name,
                    'confidence': max_confidence,
                    'decision_reason': 'pyannote_classifier',
                    'similarity': max_confidence,
                    'overlap': False
                }
            else:
                logger.info(f"ğŸ”„ í•™ìŠµëœ ë¶„ë¥˜ê¸° ì‹ ë¢°ë„ ë‚®ìŒ ({max_confidence:.3f} < 0.8), í´ë°±ìœ¼ë¡œ ì „í™˜")
                return None
        else:
            logger.warning("âš ï¸ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¼ë²¨ ë§¤í•‘ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return None
            
    except Exception as e:
        logger.error(f"âŒ pyannote.audio ë¶„ë¥˜ê¸° ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
        return None

def classify_speaker_by_text(transcript):
    """í…ìŠ¤íŠ¸ ê¸°ë°˜ í™”ì ë¶„ë¥˜ (í´ë°±)"""
    global custom_classifier, label_mapping
    
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    text = transcript.lower().strip()
    
    # í–‰ì› íŠ¹í™” í‚¤ì›Œë“œ íŒ¨í„´ (ë” êµ¬ì²´ì ìœ¼ë¡œ)
    employee_patterns = [
        r'\b(ì•ˆë…•í•˜ì„¸ìš”|ì–´ì„œì˜¤ì„¸ìš”|ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤|ê°ì‚¬í•©ë‹ˆë‹¤|ì£„ì†¡í•©ë‹ˆë‹¤)\b',
        r'\b(ê³ ê°ë‹˜|ë§ì”€|ê¶ê¸ˆ|ë¬¸ì˜|ìƒë‹´)\b',
        r'\b(ì¶”ì²œ|ì„¤ëª…|ì•ˆë‚´|í™•ì¸|ê²€í† )\b',
        r'\b(ì‹œë®¬ë ˆì´ì…˜|ê³„ì‚°|ì˜ˆìƒ|ì˜ˆì‹œ)\b',
        r'\b(ë‹¤ìŒ|ë‹¨ê³„|ì§„í–‰|ì™„ë£Œ|ì¢…ë£Œ)\b',
        r'\b(ì–´ë–¤.*ìƒí’ˆ|ê´€ì‹¬.*ìˆìœ¼ì‹ |ë„ì™€ë“œë¦´ê¹Œìš”|ë³´ì—¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤)\b',
        r'\b(ë§ëŠ”.*ìƒí’ˆ|ì¶”ì²œë“œë¦¬ê² ìŠµë‹ˆë‹¤|ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤)\b',
        r'\b(ì„œë¥˜.*ì¤€ë¹„|ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤|ê¶ê¸ˆí•œ.*ì )\b',
        r'\b(ìš°ëŒ€.*ì¡°ê±´|í˜œíƒ.*ì„¤ëª…|ê¸ˆë¦¬.*ì•ˆë‚´)\b',
        r'\b(ì–´ì„œ.*ì˜¤ì„¸ìš”|í™˜ì˜í•©ë‹ˆë‹¤|ì–´ë–»ê²Œ.*ë„ì™€ë“œë¦´ê¹Œìš”)\b',
        r'\b(ìƒí’ˆ.*ì¶”ì²œ|ë§ëŠ”.*ìƒí’ˆ|ì í•©í•œ.*ìƒí’ˆ)\b',
        r'\b(ë„ì™€ë“œë¦´ê¹Œìš”|ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤|ë„ì™€ë“œë¦´ê²Œìš”)\b',
        r'\b(ë³´ì—¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤|ë³´ì—¬ë“œë¦´ê²Œìš”|ë³´ì—¬ë“œë¦´ê¹Œìš”)\b',
        r'\b(ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤|ì„¤ëª…ë“œë¦´ê²Œìš”|ì„¤ëª…ë“œë¦´ê¹Œìš”)\b',
        r'\b(ì•ˆë‚´ë“œë¦¬ê² ìŠµë‹ˆë‹¤|ì•ˆë‚´ë“œë¦´ê²Œìš”|ì•ˆë‚´ë“œë¦´ê¹Œìš”)\b'
    ]
    
    # ê³ ê° íŠ¹í™” í‚¤ì›Œë“œ íŒ¨í„´
    customer_patterns = [
        r'\b(ê¶ê¸ˆ|ë¬¸ì˜|ì§ˆë¬¸|ì•Œê³ ì‹¶|ê¶ê¸ˆí•´)\b',
        r'\b(ê°€ì…í•˜ê³ ì‹¶|ì‹ ì²­í•˜ê³ ì‹¶|í•˜ê³ ì‹¶ì–´)\b',
        r'\b(ì–´ë–¤|ë¬´ì—‡|ì–¸ì œ|ì–´ë””|ì–¼ë§ˆ|ì™œ|ì–´ë–»ê²Œ)\b',
        r'\b(ì¢‹ì€|ë‚˜ìœ|ê´œì°®|ë§ë‚˜|ë§ì§€)\b',
        r'\b(ë„ì™€ì£¼|ì•Œë ¤ì£¼|ì„¤ëª…í•´ì£¼|ë³´ì—¬ì£¼)\b',
        r'\b(ë¹„êµ|ì°¨ì´|ì¥ì |ë‹¨ì |ì¥ë‹¨ì )\b',
        r'\b(ì¶”ì²œí•´ì£¼|ì¶”ì²œë°›|ì¶”ì²œí•˜ê³ ì‹¶)\b',
        r'\b(ì‹ ì²­|ê°€ì…|í•´ì§€|ë³€ê²½|ìˆ˜ì •)\b'
    ]
    
    # íŒ¨í„´ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
    employee_score = 0
    customer_score = 0
    
    for pattern in employee_patterns:
        if re.search(pattern, text):
            employee_score += 1
    
    for pattern in customer_patterns:
        if re.search(pattern, text):
            customer_score += 1
    
    # ì»¤ìŠ¤í…€ ëª¨ë¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©
    if custom_classifier and label_mapping:
        try:
            # ê°„ë‹¨í•œ íŠ¹ì§• ë²¡í„° ìƒì„± (í…ìŠ¤íŠ¸ ê¸¸ì´, í‚¤ì›Œë“œ ê°œìˆ˜ ë“±)
            features = np.array([
                len(text),
                employee_score,
                customer_score,
                text.count('?'),
                text.count('!'),
                text.count('.')
            ]).reshape(1, -1)
            
            prediction = custom_classifier.predict(features)[0]
            # ì‹¤ì œ ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ì‹ ë¢°ë„ ê³„ì‚°
            if best_match and best_similarity > 0:
                confidence = min(0.99, max(0.7, best_similarity))  # 0.7~0.99 ë²”ìœ„ë¡œ ì œí•œ
            else:
                confidence = 0.8  # ê¸°ë³¸ê°’
            
            # ë¼ë²¨ ë§¤í•‘ì—ì„œ í™”ì ì´ë¦„ ì°¾ê¸°
            speaker_name = None
            for name, label in label_mapping.items():
                if label == prediction:
                    speaker_name = name
                    break
            
            if speaker_name:
                return {
                    'speaker_id': f"speaker_{speaker_name}",
                    'speaker_name': speaker_name,
                    'confidence': confidence
                }
                
        except Exception as e:
            logger.error(f"âŒ ì»¤ìŠ¤í…€ ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
    
    # ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜
    if employee_score > customer_score:
        return {
            'speaker_id': 'speaker_employee',
            'speaker_name': 'employee',
            'confidence': 0.7
        }
    else:
        return {
            'speaker_id': 'speaker_customer',
            'speaker_name': 'customer',
            'confidence': 0.7
        }

def ensure_whisper_model():
    global whisper_model, WhisperModel
    if whisper_model is not None:
        return whisper_model
    if WhisperModel is None:
        logger.warning("âš ï¸ faster-whisper ë¯¸ì„¤ì¹˜ - ê²¹ì¹¨ êµ¬ê°„ STT ë¹„í™œì„±")
        return None
    try:
        # small / base ì„ íƒ ê°€ëŠ¥. ì„±ëŠ¥-ì†ë„ ê· í˜•ì„ ìœ„í•´ small ì‚¬ìš©.
        whisper_model = WhisperModel("small", device="cpu", compute_type="int8")
        logger.info("âœ… faster-whisper ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        return whisper_model
    except Exception as e:
        logger.error(f"âŒ faster-whisper ë¡œë“œ ì‹¤íŒ¨: {e}")
        whisper_model = None
        return None


def transcribe_snippet_from_base64(audio_base64: str, mime: Optional[str] = None) -> Optional[str]:
    """ê²¹ì¹¨ êµ¬ê°„ì— ëŒ€í•´ base64 ì˜¤ë””ì˜¤ë¥¼ Whisperë¡œ ì „ì‚¬.
    - ffmpegë¡œ 16kHz mono WAV ë³€í™˜ í›„ faster-whisper ì‚¬ìš©
    """
    temp_src = None
    temp_wav = None
    try:
        if not audio_base64:
            return None
        if shutil.which('ffmpeg') is None:
            logger.error("âŒ ffmpeg ë¯¸ì„¤ì¹˜ - overlap ì „ì‚¬ ë¶ˆê°€")
            return None
        model = ensure_whisper_model()
        if model is None:
            return None

        # base64 â†’ ì„ì‹œ ì†ŒìŠ¤ íŒŒì¼
        raw = base64.b64decode(audio_base64)
        ext = 'webm'
        if mime and 'ogg' in mime:
            ext = 'ogg'
        temp_src = f"/tmp/overlap_src.{ext}"
        with open(temp_src, 'wb') as f:
            f.write(raw)

        # ffmpeg ë³€í™˜
        temp_wav = "/tmp/overlap_conv.wav"
        cmd = [
            'ffmpeg', '-y', '-hide_banner', '-loglevel', 'error',
            '-i', temp_src, '-ar', '16000', '-ac', '1', '-f', 'wav', temp_wav
        ]
        subprocess.run(cmd, check=True)

        # Whisper ì „ì‚¬
        segments, info = model.transcribe(temp_wav, language="ko", vad_filter=True)
        texts = []
        for seg in segments:
            if seg and getattr(seg, 'text', ''):
                texts.append(seg.text.strip())
        transcript = " ".join(texts).strip()
        logger.info(f"ğŸ—£ï¸ overlap ì „ì‚¬ ê²°ê³¼: {transcript[:80]}{'...' if len(transcript)>80 else ''}")
        return transcript if transcript else None
    except Exception as e:
        logger.error(f"âŒ overlap ì „ì‚¬ ì‹¤íŒ¨: {e}")
        return None
    finally:
        try:
            if temp_src and os.path.exists(temp_src):
                os.remove(temp_src)
            if temp_wav and os.path.exists(temp_wav):
                os.remove(temp_wav)
        except Exception:
            pass

@app.route('/health', methods=['GET'])
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return jsonify({
        "model_type": "voice_profile_based",
        "status": "healthy",
        "voice_profiles_count": len(voice_profiles),
        "timestamp": datetime.now().isoformat()
    })

@app.route('/process-transcript', methods=['POST'])
def process_transcript():
    """ìŒì„± ì¸ì‹ ê²°ê³¼ ì²˜ë¦¬"""
    try:
        # multipart/form-dataì™€ JSON ëª¨ë‘ ì²˜ë¦¬
        if request.content_type and 'multipart/form-data' in request.content_type:
            # multipart/form-data ì²˜ë¦¬
            audio_file = request.files.get('audio')
            employee_id = request.form.get('employee_id')
            
            if not audio_file:
                return jsonify({
                    'speaker_id': 'speaker_unknown',
                    'speaker_name': 'unknown',
                    'confidence': 0.0,
                    'similarity': 0.0,
                    'decision_reason': 'no_audio',
                    'overlap': False,
                    'error': 'No audio file provided'
                })
            
            # ì˜¤ë””ì˜¤ íŒŒì¼ì„ base64ë¡œ ë³€í™˜
            audio_data = audio_file.read()
            audio_base64 = base64.b64encode(audio_data).decode('utf-8')
            audio_mime = audio_file.content_type or 'audio/webm'
            
            # Whisperë¡œ ìŒì„± ì¸ì‹
            transcript = transcribe_snippet_from_base64(audio_base64, audio_mime)
            if not transcript:
                transcript = "ìŒì„± ì¸ì‹ ì‹¤íŒ¨"
            
            logger.info(f"ğŸ“ ìŒì„± ì¸ì‹ ê²°ê³¼: {transcript}")
            
            # ìŒì„± íŠ¹ì§• ì¶”ì¶œ
            derived_features = extract_audio_features_from_base64(audio_base64, audio_mime)
            
            # WAV íŒŒì¼ë¡œ ë³€í™˜í•´ì„œ í•™ìŠµëœ ë¶„ë¥˜ê¸° ì‚¬ìš© ì‹œë„
            temp_wav_path = None
            try:
                # base64 â†’ ì„ì‹œ WAV íŒŒì¼
                raw = base64.b64decode(audio_base64)
                temp_src = f"/tmp/classifier_src.{audio_mime.split('/')[-1] if audio_mime else 'webm'}"
                with open(temp_src, 'wb') as f:
                    f.write(raw)
                
                # ffmpegë¡œ WAV ë³€í™˜
                temp_wav_path = "/tmp/classifier_input.wav"
                cmd = [
                    'ffmpeg', '-y', '-hide_banner', '-loglevel', 'error',
                    '-i', temp_src, '-ar', '16000', '-ac', '1', '-f', 'wav', temp_wav_path
                ]
                subprocess.run(cmd, check=True)
                
                # í•™ìŠµëœ ë¶„ë¥˜ê¸°ë¡œ ë¶„ë¥˜ ì‹œë„
                pyannote_result = classify_speaker_by_pyannote_embedding(temp_wav_path)
                if pyannote_result:
                    logger.info("ğŸ¯ í•™ìŠµëœ ë¶„ë¥˜ê¸° ì‚¬ìš© ì„±ê³µ")
                    result = pyannote_result
                else:
                    # í´ë°±: ê¸°ì¡´ MFCC ê¸°ë°˜ ë¶„ë¥˜
                    logger.info("ğŸ”„ í•™ìŠµëœ ë¶„ë¥˜ê¸° ì‹¤íŒ¨, MFCC ê¸°ë°˜ ë¶„ë¥˜ë¡œ í´ë°±")
                    result = classify_speaker_by_voice(transcript, derived_features, None)
                
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                if os.path.exists(temp_src):
                    os.remove(temp_src)
                if os.path.exists(temp_wav_path):
                    os.remove(temp_wav_path)
                    
            except Exception as e:
                logger.warning(f"âš ï¸ í•™ìŠµëœ ë¶„ë¥˜ê¸° ì‚¬ìš© ì‹¤íŒ¨: {e}")
                # í´ë°±: ê¸°ì¡´ MFCC ê¸°ë°˜ ë¶„ë¥˜
                result = classify_speaker_by_voice(transcript, derived_features, None)
            
        else:
            # JSON ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§)
            data = request.get_json()
            logger.info(f"ğŸ“ ë°›ì€ ë°ì´í„°: {data}")
            
            transcript = data.get('transcript', '').strip()
            audio_features = data.get('audio_features')  # ì˜¤ë””ì˜¤ íŠ¹ì§• (ì„ íƒì )
            employee_voice_profile = data.get('employee_voice_profile')  # í–‰ì› ìŒì„± í”„ë¡œí•„
            audio_base64 = data.get('audio_base64')
            audio_mime = data.get('audio_mime')
            derived_features = None
            
            if audio_base64:
                derived_features = extract_audio_features_from_base64(audio_base64, audio_mime)
                
                # WAV íŒŒì¼ë¡œ ë³€í™˜í•´ì„œ í•™ìŠµëœ ë¶„ë¥˜ê¸° ì‚¬ìš© ì‹œë„
                temp_wav_path = None
                try:
                    # base64 â†’ ì„ì‹œ WAV íŒŒì¼
                    raw = base64.b64decode(audio_base64)
                    temp_src = f"/tmp/classifier_src.{audio_mime.split('/')[-1] if audio_mime else 'webm'}"
                    with open(temp_src, 'wb') as f:
                        f.write(raw)
                    
                    # ffmpegë¡œ WAV ë³€í™˜
                    temp_wav_path = "/tmp/classifier_input.wav"
                    cmd = [
                        'ffmpeg', '-y', '-hide_banner', '-loglevel', 'error',
                        '-i', temp_src, '-ar', '16000', '-ac', '1', '-f', 'wav', temp_wav_path
                    ]
                    subprocess.run(cmd, check=True)
                    
                    # í•™ìŠµëœ ë¶„ë¥˜ê¸°ë¡œ ë¶„ë¥˜ ì‹œë„
                    pyannote_result = classify_speaker_by_pyannote_embedding(temp_wav_path)
                    if pyannote_result:
                        logger.info("ğŸ¯ í•™ìŠµëœ ë¶„ë¥˜ê¸° ì‚¬ìš© ì„±ê³µ")
                        result = pyannote_result
                    else:
                        # í´ë°±: ê¸°ì¡´ MFCC ê¸°ë°˜ ë¶„ë¥˜
                        logger.info("ğŸ”„ í•™ìŠµëœ ë¶„ë¥˜ê¸° ì‹¤íŒ¨, MFCC ê¸°ë°˜ ë¶„ë¥˜ë¡œ í´ë°±")
                        result = classify_speaker_by_voice(transcript, derived_features, None)
                    
                    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                    if os.path.exists(temp_src):
                        os.remove(temp_src)
                    if os.path.exists(temp_wav_path):
                        os.remove(temp_wav_path)
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ í•™ìŠµëœ ë¶„ë¥˜ê¸° ì‚¬ìš© ì‹¤íŒ¨: {e}")
                    # í´ë°±: ê¸°ì¡´ MFCC ê¸°ë°˜ ë¶„ë¥˜
                    result = classify_speaker_by_voice(transcript, derived_features, None)
            else:
                # í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš°
                result = classify_speaker_by_text(transcript)
                result.update({'similarity': 0.0, 'decision_reason': 'text_only', 'overlap': False})
        
        if not transcript:
            return jsonify({
                'speaker_id': 'speaker_unknown',
                'speaker_name': 'unknown',
                'confidence': 0.0,
                'similarity': 0.0,
                'decision_reason': 'no_transcript',
                'overlap': False,
                'error': 'No transcript provided'
            })
        
        # ê²¹ì¹¨ í‘œì‹œëœ ê²½ìš°, ê°€ëŠ¥í•œ ê²½ìš° Whisperë¡œ ì „ì‚¬ ì‹œë„
        if result.get('overlap') and audio_base64:
            overlap_txt = transcribe_snippet_from_base64(audio_base64, audio_mime)
            if overlap_txt:
                result['overlap_transcript'] = overlap_txt
                # ê²¹ì¹¨ì´ì§€ë§Œ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì œê³µ
        
        logger.info(f"ğŸ¯ ë¶„ë¥˜ ê²°ê³¼: {result}")
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            'speaker_id': 'speaker_unknown',
            'speaker_name': 'unknown',
            'confidence': 0.0,
            'similarity': 0.0,
            'decision_reason': 'error',
            'overlap': False,
            'error': str(e)
        }), 500

@app.route('/voice-profiles', methods=['GET'])
def get_voice_profiles():
    """í˜„ì¬ ë¡œë“œëœ ìŒì„± í”„ë¡œí•„ ì •ë³´ ë°˜í™˜"""
    return jsonify({
        'profiles': voice_profiles,
        'count': len(voice_profiles)
    })

@app.route('/reload-profiles', methods=['POST'])
def reload_voice_profiles():
    """ìŒì„± í”„ë¡œí•„ ë‹¤ì‹œ ë¡œë“œ"""
    success = load_voice_profiles()
    return jsonify({
        'success': success,
        'count': len(voice_profiles) if success else 0
    })

if __name__ == '__main__':
    logger.info("ğŸš€ Voice Profile ê¸°ë°˜ í™”ì ë¶„ë¦¬ ì„œë²„ ì‹œì‘...")
    
    # ìŒì„± í”„ë¡œí•„ ë¡œë“œ
    if load_voice_profiles():
        logger.info("âœ… Voice profiles ë¡œë“œ ì„±ê³µ")
    else:
        logger.warning("âš ï¸ Voice profiles ë¡œë“œ ì‹¤íŒ¨ - í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ë¥˜ë§Œ ì‚¬ìš©")
    
    # ì»¤ìŠ¤í…€ ëª¨ë¸ ì´ˆê¸°í™”
    if initialize_custom_model():
        logger.info("âœ… ì»¤ìŠ¤í…€ ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
    else:
        logger.info("â„¹ï¸ ì»¤ìŠ¤í…€ ëª¨ë¸ ì—†ìŒ - í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ë¥˜ë§Œ ì‚¬ìš©")
    
    logger.info("âœ… ì„œë²„ ì¤€ë¹„ ì™„ë£Œ")
    logger.info("ğŸ“Š ì‚¬ìš© ëª¨ë¸: Voice Profile + í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ë¥˜")
    logger.info("ğŸ¯ ë¶„ë¥˜ ëŒ€ìƒ: í–‰ì› vs ê³ ê° (ìŒì„± ë§¤ì¹­ ìš°ì„ )")
    logger.info("ğŸŒ ì„œë²„ ì£¼ì†Œ: http://localhost:5005")
    
    app.run(host='0.0.0.0', port=5005, debug=True)
